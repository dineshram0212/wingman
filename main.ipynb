{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wingman.core.wingman import Wingman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Wingman(model_name='llama-3.3-70b-versatile', apiKey='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.chat(\"I peed my pants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'event'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.mtype.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Whoa, that's okay, accidents happen. Do you need any help or a funny joke to lighten the mood?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wingman.core.memory import Memory\n",
    "\n",
    "memory = Memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Wingman(model_name='llama-3.3-70b-versatile', apiKey='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = \"yesterday was my birthday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.chat(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Happy belated birthday! I hope you had an amazing day and got to celebrate with loved ones. How did you spend your special day?'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtype = result.mtype.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'event'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"No birthday was day before today\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3fd306b9-30df-4504-a06f-4b3b10c7ba15']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# memory.save_memory(memory_type=mtype, docs=[f\"User: '{chat}'; Response: '{result.response}'\"])\n",
    "memory.save_memory(memory_type='test', docs=[f\"User: '{chat}'; Response: '{test}'\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 February 2025,  05:16 PM\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "x = datetime.datetime.now()\n",
    "\n",
    "print(x.strftime(\"%d %B %Y,  %I:%M %p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = \"When is my birthday yesterdat?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short\n"
     ]
    }
   ],
   "source": [
    "print(client.classify_memory(chat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrvd_memory = memory.load_memory(chat, memory_type='test', min_score=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = client.chat(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseFormatter(response=\"I'm not aware of your birthday details. Could you please provide more context or information about your birthday?\", mtype=<MemoryType.short: 'short'>)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wingman.core.wingman import Wingman\n",
    "\n",
    "client = Wingman(model_name='llama-3.3-70b-versatile', apiKey='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Type:  event\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResponseFormatter(response='Yes, I remember you told me yesterday was your birthday. Happy belated birthday again!', mtype='long')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.chat(\"Do you remember my birthday?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Type:  event\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResponseFormatter(response='Your birthday was not on yesterday, but since we were talking about your birthday earlier, I will remember that your birthday is today, February 28, 2025.', mtype='long')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.chat(\"No my birthday was not on yesterday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "model = OllamaLLM(model='llama3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage\n",
    "builder = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.add_edge(START, \"model\")\n",
    "builder.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "query = \"No my name is Ram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello Ram! Nice to meet you too! Don't worry, it can be easy for names and conversations to get jumbled up. Let's start fresh - how are you doing today? Is everything okay?\n"
     ]
    }
   ],
   "source": [
    "input_messages = [HumanMessage(query)]\n",
    "output = graph.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wingman.core.memory import Memory\n",
    "\n",
    "memory = Memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wingman.core.wingman import Wingman\n",
    "\n",
    "client = Wingman(model_name='qwen-2.5-32b', apiKey='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': 'Whats my name'}\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Whats my name\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from langgraph.graph import MessagesState\n",
    "chat_message = \"Whats my name\"\n",
    "\n",
    "state = MessagesState(messages=chat_message)\n",
    "config= {\"configurable\": {\n",
    "            \"thread_id\": datetime.datetime.now().strftime('%d%m%Y'),\n",
    "        }}\n",
    "print(state)\n",
    "client.chat(state, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wingman.core.wingman import Wingman\n",
    "from langchain_core.messages import HumanMessage\n",
    "import datetime\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# Initialize the client\n",
    "client = Wingman(model_name='qwen-2.5-32b', apiKey='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='I want it as object oriented program', additional_kwargs={}, response_metadata={})]}\n",
      "Loaded Messages:  [HumanMessage(content='Whats my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Dinesh.', additional_kwargs={}, response_metadata={}), HumanMessage(content='give me a python code to print if a year is leap year or not', additional_kwargs={}, response_metadata={}), AIMessage(content='Sure, here\\'s a simple Python code snippet to determine if a given year is a leap year or not:\\n\\n```python\\ndef is_leap_year(year):\\n    if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\\n        return True\\n    else:\\n        return False\\n\\n# Example usage\\nyear = 2024\\nif is_leap_year(year):\\n    print(f\"{year} is a leap year.\")\\nelse:\\n    print(f\"{year} is not a leap year.\")\\n```\\nYou can replace the `year` variable with any year you want to check.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I want it as object oriented program', additional_kwargs={}, response_metadata={}, id='203a6d35-26a8-45a7-9b64-7c4653bae102')]\n",
      "\n",
      "Wingman's response:\n",
      "Sure, let's convert that into an object-oriented program. Here's how you can do it:\n",
      "\n",
      "```python\n",
      "class LeapYearChecker:\n",
      "    def __init__(self, year):\n",
      "        self.year = year\n",
      "\n",
      "    def is_leap_year(self):\n",
      "        if (self.year % 4 == 0 and self.year % 100 != 0) or (self.year % 400 == 0):\n",
      "            return True\n",
      "        else:\n",
      "            return False\n",
      "\n",
      "# Example usage\n",
      "year = 2024\n",
      "checker = LeapYearChecker(year)\n",
      "if checker.is_leap_year():\n",
      "    print(f\"{year} is a leap year.\")\n",
      "else:\n",
      "    print(f\"{year} is not a leap year.\")\n",
      "```\n",
      "You can modify the `year` variable to test different years.\n"
     ]
    }
   ],
   "source": [
    "# Create a proper message state\n",
    "chat_message = \"I want it as object oriented program\"\n",
    "state = MessagesState(messages=[HumanMessage(content=chat_message)])\n",
    "\n",
    "# Configure with thread_id\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": datetime.datetime.now().strftime('%d%m%Y'),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print the state\n",
    "print(state)\n",
    "\n",
    "# Call the chat method and print the response\n",
    "response = client.chat(state, config=config)\n",
    "print(\"\\nWingman's response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='Write a python code to predict if leap year or not.', additional_kwargs={}, response_metadata={}, id='30f3592f-8d2a-48e3-a8b0-2ca801bbbf1d'), ResponseFormatter(response=\"Here's a simple Python function to determine if a year is a leap year or not:\\n\\ndef is_leap_year(year):\\n    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\\n\\n# Example usage:\\nyear = 2024\\nif is_leap_year(year):\\n    print(f'{year} is a leap year.')\\nelse:\\n    print(f'{year} is not a leap year.')\", mtype='none'), HumanMessage(content='Give it as object oriented program.', additional_kwargs={}, response_metadata={}, id='18c29e85-c9df-402b-8bc8-7e20a702522f'), ResponseFormatter(response=\"Here's the equivalent object-oriented Python program to determine if a year is a leap year or not:\\n\\nclass Year:\\n    def __init__(self, year):\\n        self.year = year\\n\\n    def is_leap_year(self):\\n        return self.year % 4 == 0 and (self.year % 100 != 0 or self.year % 400 == 0)\\n\\n# Example usage:\\nyear = Year(2024)\\nif year.is_leap_year():\\n    print(f'{year.year} is a leap year.')\\nelse:\\n    print(f'{year.year} is not a leap year.')\", mtype='none')])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_chat_history(datetime.datetime.now().strftime('%d%m%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'04032025': InMemoryChatMessageHistory(messages=[HumanMessage(content='Whats my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Dinesh.', additional_kwargs={}, response_metadata={}), HumanMessage(content='give me a python code to print if a year is leap year or not', additional_kwargs={}, response_metadata={}), AIMessage(content='Sure, here\\'s a simple Python code snippet to determine if a given year is a leap year or not:\\n\\n```python\\ndef is_leap_year(year):\\n    if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\\n        return True\\n    else:\\n        return False\\n\\n# Example usage\\nyear = 2024\\nif is_leap_year(year):\\n    print(f\"{year} is a leap year.\")\\nelse:\\n    print(f\"{year} is not a leap year.\")\\n```\\nYou can replace the `year` variable with any year you want to check.', additional_kwargs={}, response_metadata={})])}\n"
     ]
    }
   ],
   "source": [
    "print(client.chats_by_thread_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from wingman.plugins.email_tool import send_email\n",
    "tools = [send_email]\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model_name='llama-3.3-70b-versatile', api_key='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').bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"write an email to balaragavesh@gmail.com inviting him for a birthday party\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'call_9293',\n",
       "  'function': {'arguments': '{\"subject\": \"Birthday Party Invitation\", \"body\": \"Dear Balaragavesh, you are cordially invited to a birthday party. Please join us to celebrate this special day. Date, time, and venue will be shared shortly. Looking forward to seeing you there!\", \"recipient_email\": \"balaragavesh@gmail.com\"}',\n",
       "   'name': 'send_email'},\n",
       "  'type': 'function'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.additional_kwargs['tool_calls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email sent successfully to balaragavesh@gmail.com\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='write an email to balaragavesh@gmail.com inviting him for a birthday party', additional_kwargs={}, response_metadata={}, id='d02fd997-43bc-4ff1-93f4-ab823b1622cc'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ecnx', 'function': {'arguments': '{\"subject\": \"Birthday Party Invitation\", \"body\": \"Dear Balaragavesh, you are cordially invited to a birthday party. Please join us to celebrate this special day. Date, time, and venue will be shared separately. Looking forward to seeing you there!\", \"recipient_email\": \"balaragavesh@gmail.com\"}', 'name': 'send_email'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 247, 'total_tokens': 325, 'completion_time': 0.283636364, 'prompt_time': 0.016363943, 'queue_time': 0.518375041, 'total_time': 0.300000307}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_76dc6cf67d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e37ace40-3e0a-4dbc-aa84-0f54f10c9909-0', tool_calls=[{'name': 'send_email', 'args': {'subject': 'Birthday Party Invitation', 'body': 'Dear Balaragavesh, you are cordially invited to a birthday party. Please join us to celebrate this special day. Date, time, and venue will be shared separately. Looking forward to seeing you there!', 'recipient_email': 'balaragavesh@gmail.com'}, 'id': 'call_ecnx', 'type': 'tool_call'}], usage_metadata={'input_tokens': 247, 'output_tokens': 78, 'total_tokens': 325}),\n",
       "  ToolMessage(content='null', name='send_email', id='4ff369f6-6932-4caf-8fbb-4acd3eecb6b3', tool_call_id='call_ecnx'),\n",
       "  AIMessage(content='I hope the email is sent successfully. Let me know if you need any further assistance.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 336, 'total_tokens': 355, 'completion_time': 0.070840555, 'prompt_time': 0.018812469, 'queue_time': 0.32188425, 'total_time': 0.089653024}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_e669a124b2', 'finish_reason': 'stop', 'logprobs': None}, id='run-429f7728-ecba-4f16-9bbd-9110325aeddd-0', usage_metadata={'input_tokens': 336, 'output_tokens': 19, 'total_tokens': 355})]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = \"write an email to balaragavesh@gmail.com inviting him for a birthday party\"\n",
    "\n",
    "app.invoke(MessagesState(messages=chat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Langgraph with tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wingman.core.wingman import Wingman\n",
    "\n",
    "client = Wingman(model_name='llama-3.3-70b-versatile', apiKey='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query to be searched:\n",
      " Do you know my name?\n",
      "Searching for query - Do you know my name? in long_term.\n",
      "Ending workflow\n",
      "Wingman's Response:\n",
      " No, I don't know your name. You never told me.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from langgraph.graph import  MessagesState\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_message = \"Do you know my name?\"\n",
    "state = MessagesState(messages=[HumanMessage(content=chat_message)])\n",
    "\n",
    "# Configure with thread_id\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": datetime.datetime.now().strftime('%d%m%Y'),\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "result = client.chat(state, config = config)\n",
    "print(\"Wingman's Response:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You inquired about how someone was addressed, but the conversation took a different turn and an email was sent to \"balaragavesh@gmail.com\" with an invitation to the 2025 Tech Conference. The email included details about the conference, such as keynote speeches, panel discussions, and networking opportunities. Unfortunately, the initial question about how someone was addressed was not directly answered in the conversation.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='My brothers name is Yokesh', additional_kwargs={}, response_metadata={})]}\n",
      "Searching for query - My brothers name is Yokesh in long_term.\n",
      "[HumanMessage(content='My brothers name is Yokesh', additional_kwargs={}, response_metadata={})]\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_7tnq', 'function': {'arguments': '{\"chat\":\"My brothers name is Yokesh\",\"mtype\":\"long_term\"}', 'name': 'save_recall_memory'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 85, 'prompt_tokens': 1557, 'total_tokens': 1642, 'completion_time': 0.257575758, 'prompt_time': 0.054265424, 'queue_time': 0.4973786, 'total_time': 0.311841182}, 'model_name': 'mistral-saba-24b', 'system_fingerprint': 'fp_1ac1b10b93', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-5b489198-2000-43d1-9b3f-386add82c4c1-0' tool_calls=[{'name': 'save_recall_memory', 'args': {'chat': 'My brothers name is Yokesh', 'mtype': 'long_term'}, 'id': 'call_7tnq', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1557, 'output_tokens': 85, 'total_tokens': 1642}\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from langgraph.graph import  MessagesState\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_message = \"My brothers name is Yokesh\"\n",
    "state = MessagesState(messages=[HumanMessage(content=chat_message)])\n",
    "\n",
    "# Configure with thread_id\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": datetime.datetime.now().strftime('%d%m%Y'),\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "result = client.call_model(state, config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To send an email to Mr. Zang, I would need to know the subject and body of the email. Could you please provide me with that information? Additionally, I would need Mr. Zang's email address to send the email to him. \\n\\nPlease provide the required details, and I'll be happy to assist you further.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wingman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
